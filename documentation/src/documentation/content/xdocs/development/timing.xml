<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">

<document>
<header>
  <title>How The Grinder records test times - a problem?</title>
</header>

<body>

<section>
  <title>Timing tests</title>

  <p>The Grinder records test times for each successful test. By
  default, this is done by a section of code that looks like:</p>

  <source>
m_context.startTimer(); // Critical section starts

try {
    // do test
}
finally {
    m_context.stopTimer();  // Critical section ends
}
</source>

  <p>This is repeated for each test.</p>

  <p>If there are many threads within the worker process, (and the
  sleep time is small or the test takes a long time or the test
  performs I/O), it is <em>highly</em> likely that the JVM will swap
  the thread out in the critical section causing an erroneously large
  test time to be reported.</p>

  <p>Similarly, if the host machine that you are running The Grinder
  on is also running other active processes (such as other worker
  processes), it is <em>highly</em> likely that the JVM will swap the
  process out in the critical section, again causing an erroneously
  large test time to be reported. If The Grinder is co-hosted with the
  target server, and the plug-in uses a synchronous protocol, (e.g.
  the HTTP plug-in), such swapping is a <em>certainty</em>.</p>

  <p>Further, as the CPU utilisation rises the contention on the
  critical section rises non-linearly and in a way that is difficult
  to quantify. The recorded time becomes more a measure of how the OS
  and JVM can swap between multiple threads and less a measure of
  server performance.</p>

  <p><em>This is a generic problem with all test harnesses and is not
  limited to The Grinder or Java</em>. Within the scope of a single
  machine there is little that can be done about this whilst
  realistically using multiple threads and processes.</p>

  <section>
    <title>Fiddling with Thread scheduling - a partial fix</title>

    <p>From The Grinder 2.6.1, the call to <code>startTimer</code>
    makes a <code>Thread.yield()</code> call before recording the
    start time which means that a thread is more likely to be swapped
    out/in just before the critical sections. It dramatically reduced
    the response times I measured (e.g. 30 ms to 3 ms). I consider
    this only an approximate fix to the problem - it does not prevent
    the OS from swapping the process out.</p>

    <p>The recorded response time should always be considered an upper
    bound on the actual response time. Doing the <code>yield()</code>
    makes that bound more accurate.</p>

    <p>An argument against doing this is that it slightly alters the
    statistical distribution of the client invocations. I'd counter
    that without the <code>yield()</code> the distribution is not
    even; its down to the OS and JVM scheduling so threads/processes
    are far likely to be swapped at some points (e.g. waiting on I/O)
    than others. Because of this I decided there is little point in
    making the <code>yield()</code> optional.</p>
  </section>

  <section>
    <title>The <em>Timer Client</em> model - a solution?</title>

    <p>One solution to this problem is to dedicate a single machine to
    the measuring of response times. I call this the "timer client"
    model.</p>

    <p>As of The Grinder 2.6.1 you can set a property (<a
    href="site:g2/getting-started/properties">grinder.recordTime</a> for
    The Grinder 2, renamed to
    <a href="site:g3/getting-started/properties">grinder.reportTimesToConsole</a>
    for The Grinder 3) to be
    <code>false</code> which will cause the worker processes that use
    that <code>grinder.properties</code> file not report the test
    times to the console. You should run all but one
    of your worker processes with this property set to
    <code>false</code>. These are the <em>load clients</em>.</p>

    <p>You should copy the grinder.properties file to a dedicated
    <em>timer client</em> machine, change the
    <code>grinder.recordTime</code>/<code>grinder.reportTimesToConsole</code>
    property to be <code>true</code>, and set <a
    href="site:g2/getting-started/properties">grinder.processes</a> and <a
    href="site:g2/getting-started/properties">grinder.threads</a> to <code>1</code>.
    The single worker process will run on the timing client, record
    all timing information and (optionally) report it to the console.
    The less other stuff you run on the timing client, the better.</p>

    <p>The disadvantage of this method is that the statistical sample
    of the test times is much smaller.</p>
  </section>

  <section>
    <title>An update after some experimentation</title>

    <p>Testing has shown that the difference the timer client model
    makes is only measurable when those clients are co-hosted with the
    server. When you have separate server and client machines its
    better to not use the timer client model because it decreases the
    sample size of test times.</p>

    <p>I recommend trying both models. If you discover something
    interesting, please report it to <a
    href="ext:mail/grinder-use">grinder-use</a>.</p>
  </section>
</section>

</body>
</document>
